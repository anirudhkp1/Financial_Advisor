{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b7f3e8a",
      "metadata": {
        "id": "7b7f3e8a"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir \\\n",
        "    numpy==1.24.4 \\\n",
        "    scipy==1.11.4 \\\n",
        "    scikit-learn==1.4.2 \\\n",
        "    pandas==2.2.2 \\\n",
        "    matplotlib==3.8.4 \\\n",
        "    seaborn==0.13.2\n",
        "\n",
        "# 3️⃣ Install LangChain, OpenAI SDK, and AutoGen (latest stable versions)\n",
        "!pip install --no-cache-dir \\\n",
        "    'langchain<0.2.0' \\\n",
        "    'langchain-openai' \\\n",
        "    'langchain-community' \\\n",
        "    'openai' \\\n",
        "    'autogen-agentchat' \\\n",
        "    'autogen-ext[openai,azure]' \\\n",
        "    python-dotenv openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49bfaefc",
      "metadata": {
        "id": "49bfaefc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
        "from langchain.agents import create_sql_agent\n",
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.agents import UserProxyAgent\n",
        "from autogen_agentchat.teams import SelectorGroupChat\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "import asyncio\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.svm import OneClassSVM\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5654b4",
      "metadata": {
        "id": "ec5654b4"
      },
      "outputs": [],
      "source": [
        "api_key = userdata.get('openai_api_key')\n",
        "\n",
        "model_client = OpenAIChatCompletionClient(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=api_key,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e11c9bf",
      "metadata": {
        "id": "1e11c9bf"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    openai_api_key=api_key,\n",
        "    temperature=0.1,  # Slightly higher temperature\n",
        "    max_tokens=1000   # Ensure enough tokens for responses\n",
        ")\n",
        "\n",
        "db = SQLDatabase.from_uri(\"sqlite:///mydb.db\")\n",
        "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
        "\n",
        "sql_agent = create_sql_agent(\n",
        "        llm=llm,\n",
        "        toolkit=toolkit,\n",
        "        verbose=True,\n",
        "        agent_type=\"openai-functions\",\n",
        "        handle_parsing_errors=True,\n",
        "        max_iterations=20,\n",
        "        early_stopping_method=\"generate\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query_db(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Query the database using the SQL agent.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's question or request about the data\n",
        "\n",
        "    Returns:\n",
        "        str: The result from the database query\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add context about the transactions table\n",
        "        enhanced_query = f\"{query}. Use the transactions table for this query.\"\n",
        "        result = sql_agent.run(enhanced_query)\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"Error querying database: {str(e)}\""
      ],
      "metadata": {
        "id": "kvY2Wm5_CMd6"
      },
      "id": "kvY2Wm5_CMd6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SQLAnomalyDetector:\n",
        "    def __init__(self, db_path=\"mydb.db\"):\n",
        "        self.db_path = db_path\n",
        "        self.conn = sqlite3.connect(db_path)\n",
        "        self.df = None\n",
        "\n",
        "    def load_data(self, query=None):\n",
        "        \"\"\"Load data from SQL database with improved date/time handling\"\"\"\n",
        "        if query is None:\n",
        "            query = \"SELECT * FROM transactions\"\n",
        "\n",
        "        self.df = pd.read_sql_query(query, self.conn)\n",
        "\n",
        "        # Print column information for debugging\n",
        "        print(f\"Loaded {len(self.df)} records\")\n",
        "        print(f\"Columns: {list(self.df.columns)}\")\n",
        "        if len(self.df) > 0:\n",
        "            print(f\"Column types: {self.df.dtypes.to_dict()}\")\n",
        "\n",
        "        # Handle your specific date/time format\n",
        "        datetime_created = False\n",
        "\n",
        "        # Your data has 'Date' and 'Time' columns in format: '01-Mar-22' and '12:00 AM'\n",
        "        if 'Date' in self.df.columns and 'Time' in self.df.columns:\n",
        "            try:\n",
        "                print(\"Attempting to combine Date and Time columns...\")\n",
        "                if len(self.df) > 0:\n",
        "                    print(f\"Sample Date: {self.df['Date'].iloc[0]}\")\n",
        "                    print(f\"Sample Time: {self.df['Time'].iloc[0]}\")\n",
        "\n",
        "                # Combine date and time strings\n",
        "                combined_strings = self.df['Date'].astype(str) + ' ' + self.df['Time'].astype(str)\n",
        "\n",
        "                # Try different parsing approaches for your specific format\n",
        "                try:\n",
        "                    # First try: assume format like '01-Mar-22 12:00 AM'\n",
        "                    self.df['DateTime'] = pd.to_datetime(combined_strings, format='%d-%b-%y %I:%M %p', errors='coerce')\n",
        "                except:\n",
        "                    try:\n",
        "                        # Second try: let pandas infer the format\n",
        "                        self.df['DateTime'] = pd.to_datetime(combined_strings, errors='coerce')\n",
        "                    except:\n",
        "                        # Third try: manual parsing\n",
        "                        self.df['DateTime'] = pd.to_datetime(combined_strings, infer_datetime_format=True, errors='coerce')\n",
        "\n",
        "                # Check if conversion was successful\n",
        "                valid_dates = self.df['DateTime'].notna().sum()\n",
        "                if valid_dates > 0:\n",
        "                    datetime_created = True\n",
        "                    print(f\"✓ Successfully created DateTime from Date and Time columns\")\n",
        "                    print(f\"✓ {valid_dates}/{len(self.df)} valid dates created\")\n",
        "                    print(f\"Date range: {self.df['DateTime'].min()} to {self.df['DateTime'].max()}\")\n",
        "                else:\n",
        "                    print(\"✗ Failed to parse Date and Time columns\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error combining Date and Time: {e}\")\n",
        "\n",
        "        # Fallback: try to find other datetime columns\n",
        "        if not datetime_created:\n",
        "            print(\"⚠ Could not create DateTime from Date/Time columns\")\n",
        "            print(\"⚠ Time-based anomaly detection will be skipped\")\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def statistical_outliers(self, column, method='z_score', threshold=3):\n",
        "        \"\"\"Detect outliers using statistical methods\"\"\"\n",
        "        if self.df is None:\n",
        "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
        "\n",
        "        if column not in self.df.columns:\n",
        "            raise ValueError(f\"Column '{column}' not found in data\")\n",
        "\n",
        "        data = self.df[column].dropna()\n",
        "\n",
        "        if len(data) == 0:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Convert to numeric if possible\n",
        "        try:\n",
        "            data = pd.to_numeric(data, errors='coerce').dropna()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        if len(data) == 0:\n",
        "            print(f\"Warning: No numeric data found in column '{column}'\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        if method == 'z_score':\n",
        "            if data.std() == 0:\n",
        "                return pd.DataFrame()\n",
        "            z_scores = np.abs((data - data.mean()) / data.std())\n",
        "            outliers = z_scores > threshold\n",
        "\n",
        "        elif method == 'iqr':\n",
        "            Q1 = data.quantile(0.25)\n",
        "            Q3 = data.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            if IQR == 0:\n",
        "                return pd.DataFrame()\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            outliers = (data < lower_bound) | (data > upper_bound)\n",
        "\n",
        "        elif method == 'modified_z_score':\n",
        "            median = data.median()\n",
        "            mad = np.median(np.abs(data - median))\n",
        "            if mad == 0:\n",
        "                return pd.DataFrame()\n",
        "            modified_z_scores = 0.6745 * (data - median) / mad\n",
        "            outliers = np.abs(modified_z_scores) > threshold\n",
        "\n",
        "        return self.df[self.df.index.isin(data[outliers].index)]\n",
        "\n",
        "    def isolation_forest_detection(self, columns, contamination=0.1):\n",
        "        \"\"\"Isolation Forest - Good for high-dimensional data\"\"\"\n",
        "        if self.df is None:\n",
        "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
        "\n",
        "        # Check if columns exist\n",
        "        missing_cols = [col for col in columns if col not in self.df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Columns not found: {missing_cols}\")\n",
        "\n",
        "        # Select only numeric columns\n",
        "        numeric_cols = []\n",
        "        for col in columns:\n",
        "            try:\n",
        "                numeric_data = pd.to_numeric(self.df[col], errors='coerce')\n",
        "                if numeric_data.notna().sum() > 0:\n",
        "                    numeric_cols.append(col)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if not numeric_cols:\n",
        "            print(\"Warning: No numeric columns found for isolation forest\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        X = self.df[numeric_cols].copy()\n",
        "\n",
        "        # Convert to numeric and handle missing values\n",
        "        for col in numeric_cols:\n",
        "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "\n",
        "        X = X.dropna()\n",
        "\n",
        "        if len(X) == 0:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
        "        outliers = iso_forest.fit_predict(X_scaled)\n",
        "\n",
        "        anomaly_mask = outliers == -1\n",
        "        return self.df.loc[X.index[anomaly_mask]]\n",
        "\n",
        "    def financial_transaction_anomalies(self):\n",
        "        \"\"\"Detect financial transaction specific anomalies\"\"\"\n",
        "        if self.df is None:\n",
        "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
        "\n",
        "        anomalies = {}\n",
        "\n",
        "        # Find amount columns with different possible names\n",
        "\n",
        "        amount_cols = []\n",
        "        for col in ['Amount', 'amount', 'AMOUNT', 'Cash Out', 'cash_out', 'Cash In', 'cash_in']:\n",
        "            if col in self.df.columns:\n",
        "                amount_cols.append(col)\n",
        "\n",
        "        # 1. Large cash withdrawals\n",
        "        for col in ['Cash Out', 'cash_out', 'withdrawal', 'debit']:\n",
        "            if col in self.df.columns:\n",
        "                try:\n",
        "                    cash_out_data = pd.to_numeric(self.df[col], errors='coerce')\n",
        "                    if cash_out_data.notna().sum() > 0 and (cash_out_data > 0).sum() > 0:\n",
        "                        large_withdrawals = self.statistical_outliers(col, method='iqr')\n",
        "                        if len(large_withdrawals) > 0:\n",
        "                            anomalies['large_withdrawals'] = large_withdrawals\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # 2. Large cash deposits\n",
        "        for col in ['Cash In', 'cash_in', 'deposit', 'credit']:\n",
        "            if col in self.df.columns:\n",
        "                try:\n",
        "                    cash_in_data = pd.to_numeric(self.df[col], errors='coerce')\n",
        "                    if cash_in_data.notna().sum() > 0 and (cash_in_data > 0).sum() > 0:\n",
        "                        large_deposits = self.statistical_outliers(col, method='iqr')\n",
        "                        if len(large_deposits) > 0:\n",
        "                            anomalies['large_deposits'] = large_deposits\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # 3. Unusual amounts\n",
        "        for col in ['Amount', 'amount', 'AMOUNT', 'transaction_amount']:\n",
        "            if col in self.df.columns:\n",
        "                try:\n",
        "                    unusual_amounts = self.statistical_outliers(col, method='iqr')\n",
        "                    if len(unusual_amounts) > 0:\n",
        "                        anomalies['unusual_amounts'] = unusual_amounts\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # 4. Frequency anomalies\n",
        "        if 'DateTime' in self.df.columns and self.df['DateTime'].notna().sum() > 0:\n",
        "            try:\n",
        "                daily_counts = self.df.groupby(self.df['DateTime'].dt.date).size()\n",
        "                if len(daily_counts) > 1:\n",
        "                    freq_threshold = daily_counts.mean() + 2 * daily_counts.std()\n",
        "                    high_freq_days = daily_counts[daily_counts > freq_threshold]\n",
        "                    if len(high_freq_days) > 0:\n",
        "                        high_freq_transactions = self.df[self.df['DateTime'].dt.date.isin(high_freq_days.index)]\n",
        "                        anomalies['high_frequency_days'] = high_freq_transactions\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not analyze frequency patterns: {e}\")\n",
        "\n",
        "        # Weekday vs Weekend analysis - only if DateTime exists\n",
        "        if 'DateTime' in self.df.columns and self.df['DateTime'].notna().sum() > 0:\n",
        "            try:\n",
        "                # Add day of week (0=Monday, 6=Sunday)\n",
        "                self.df['day_of_week'] = self.df['DateTime'].dt.dayofweek\n",
        "                self.df['is_weekend'] = self.df['day_of_week'].isin([5, 6])  # Saturday=5, Sunday=6\n",
        "\n",
        "                # Count transactions by weekday vs weekend\n",
        "                weekday_count = (~self.df['is_weekend']).sum()\n",
        "                weekend_count = self.df['is_weekend'].sum()\n",
        "\n",
        "                # Calculate expected weekend ratio (2/7 ≈ 0.286)\n",
        "                expected_weekend_ratio = 2/7\n",
        "                actual_weekend_ratio = weekend_count / (weekday_count + weekend_count)\n",
        "\n",
        "                # If weekend transactions are significantly higher than expected, flag as anomaly\n",
        "                if actual_weekend_ratio > expected_weekend_ratio * 1.5:  # 50% higher than expected\n",
        "                    weekend_transactions = self.df[self.df['is_weekend']]\n",
        "                    anomalies['excessive_weekend_activity'] = weekend_transactions\n",
        "\n",
        "                # Also check for unusual weekend transaction amounts\n",
        "                if weekend_count > 0 and weekday_count > 0:\n",
        "                    # Find amount columns and compare weekend vs weekday patterns\n",
        "                    for col in amount_cols:\n",
        "                        if col in self.df.columns:\n",
        "                            try:\n",
        "                                amounts = pd.to_numeric(self.df[col], errors='coerce')\n",
        "                                if amounts.notna().sum() > 0:\n",
        "                                    weekend_amounts = amounts[self.df['is_weekend']].dropna()\n",
        "                                    weekday_amounts = amounts[~self.df['is_weekend']].dropna()\n",
        "\n",
        "                                    if len(weekend_amounts) > 0 and len(weekday_amounts) > 0:\n",
        "                                        # Compare median amounts\n",
        "                                        weekend_median = weekend_amounts.median()\n",
        "                                        weekday_median = weekday_amounts.median()\n",
        "\n",
        "                                        # If weekend median is significantly higher, flag transactions\n",
        "                                        if weekend_median > weekday_median * 1.5:\n",
        "                                            high_weekend_amounts = self.df[\n",
        "                                                self.df['is_weekend'] &\n",
        "                                                (amounts > weekend_amounts.quantile(0.75))\n",
        "                                            ]\n",
        "                                            if len(high_weekend_amounts) > 0:\n",
        "                                                anomalies['high_weekend_amounts'] = high_weekend_amounts\n",
        "                                    break\n",
        "                            except:\n",
        "                                continue\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not analyze weekday/weekend patterns: {e}\")\n",
        "\n",
        "        return anomalies\n",
        "\n",
        "    def comprehensive_analysis(self, amount_col='Amount', cash_in_col='Cash In', cash_out_col='Cash Out'):\n",
        "        \"\"\"Run comprehensive anomaly detection analysis for financial transactions\"\"\"\n",
        "        if self.df is None:\n",
        "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        print(\"🔍 Running Comprehensive Financial Anomaly Detection Analysis...\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check available columns based on your dataset\n",
        "        available_cols = []\n",
        "        column_mapping = {\n",
        "            'Amount': amount_col,\n",
        "            'Cash In': cash_in_col,\n",
        "            'Cash Out': cash_out_col\n",
        "        }\n",
        "\n",
        "        for display_name, col_name in column_mapping.items():\n",
        "            if col_name in self.df.columns:\n",
        "                available_cols.append(col_name)\n",
        "                print(f\"✓ Found column: {col_name}\")\n",
        "            else:\n",
        "                print(f\"✗ Column not found: {col_name}\")\n",
        "\n",
        "        if not available_cols:\n",
        "            print(f\"❌ None of the specified columns found in data\")\n",
        "            print(f\"Available columns: {list(self.df.columns)}\")\n",
        "            return results\n",
        "\n",
        "        # 1. Amount anomalies (both positive and negative values)\n",
        "        if amount_col in self.df.columns:\n",
        "            print(f\"\\n1. Amount Anomalies Analysis ({amount_col})\")\n",
        "            try:\n",
        "                # Z-score method\n",
        "                z_anomalies = self.statistical_outliers(amount_col, method='z_score')\n",
        "                results['amount_z_score'] = z_anomalies\n",
        "                print(f\"   Z-Score outliers: {len(z_anomalies)} anomalies\")\n",
        "\n",
        "                # IQR method\n",
        "                iqr_anomalies = self.statistical_outliers(amount_col, method='iqr')\n",
        "                results['amount_iqr'] = iqr_anomalies\n",
        "                print(f\"   IQR outliers: {len(iqr_anomalies)} anomalies\")\n",
        "\n",
        "                # Show some stats\n",
        "                amount_data = pd.to_numeric(self.df[amount_col], errors='coerce').dropna()\n",
        "                if len(amount_data) > 0:\n",
        "                    print(f\"   Amount range: {amount_data.min()} to {amount_data.max()}\")\n",
        "                    print(f\"   Amount mean: {amount_data.mean():.2f}, std: {amount_data.std():.2f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Error analyzing amounts: {e}\")\n",
        "\n",
        "        # 2. Cash In anomalies (deposits)\n",
        "        if cash_in_col in self.df.columns:\n",
        "            print(f\"\\n2. Cash In Anomalies Analysis ({cash_in_col})\")\n",
        "            try:\n",
        "                # Only analyze non-zero cash in values\n",
        "                cash_in_data = pd.to_numeric(self.df[cash_in_col], errors='coerce')\n",
        "                non_zero_mask = cash_in_data > 0\n",
        "\n",
        "                if non_zero_mask.sum() > 0:\n",
        "                    cash_in_anomalies = self.statistical_outliers(cash_in_col, method='iqr')\n",
        "                    results['cash_in_anomalies'] = cash_in_anomalies\n",
        "                    print(f\"   Found {len(cash_in_anomalies)} large deposit anomalies\")\n",
        "                    print(f\"   Non-zero deposits: {non_zero_mask.sum()}/{len(self.df)} transactions\")\n",
        "                else:\n",
        "                    print(f\"   No cash deposits found in {cash_in_col}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Error analyzing cash in: {e}\")\n",
        "\n",
        "        # 3. Cash Out anomalies (withdrawals)\n",
        "        if cash_out_col in self.df.columns:\n",
        "            print(f\"\\n3. Cash Out Anomalies Analysis ({cash_out_col})\")\n",
        "            try:\n",
        "                # Only analyze non-zero cash out values\n",
        "                cash_out_data = pd.to_numeric(self.df[cash_out_col], errors='coerce')\n",
        "                non_zero_mask = cash_out_data > 0\n",
        "\n",
        "                if non_zero_mask.sum() > 0:\n",
        "                    cash_out_anomalies = self.statistical_outliers(cash_out_col, method='iqr')\n",
        "                    results['cash_out_anomalies'] = cash_out_anomalies\n",
        "                    print(f\"   Found {len(cash_out_anomalies)} large withdrawal anomalies\")\n",
        "                    print(f\"   Non-zero withdrawals: {non_zero_mask.sum()}/{len(self.df)} transactions\")\n",
        "                else:\n",
        "                    print(f\"   No cash withdrawals found in {cash_out_col}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Error analyzing cash out: {e}\")\n",
        "\n",
        "        # 4. Multi-column isolation forest\n",
        "        print(f\"\\n4. Multi-dimensional Analysis (Isolation Forest)\")\n",
        "        try:\n",
        "            numeric_cols = []\n",
        "            for col in available_cols:\n",
        "                try:\n",
        "                    test_data = pd.to_numeric(self.df[col], errors='coerce')\n",
        "                    if test_data.notna().sum() > 0:\n",
        "                        numeric_cols.append(col)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if len(numeric_cols) >= 2:\n",
        "                iso_anomalies = self.isolation_forest_detection(numeric_cols)\n",
        "                results['isolation_forest'] = iso_anomalies\n",
        "                print(f\"   Using columns: {numeric_cols}\")\n",
        "                print(f\"   Found {len(iso_anomalies)} multi-dimensional anomalies\")\n",
        "            else:\n",
        "                print(f\"   Need at least 2 numeric columns. Found: {numeric_cols}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Error in isolation forest: {e}\")\n",
        "\n",
        "        # 5. Transaction pattern anomalies\n",
        "        print(f\"\\n5. Transaction Pattern Analysis\")\n",
        "        try:\n",
        "            # Category-based anomalies\n",
        "            if 'Category' in self.df.columns:\n",
        "                category_counts = self.df['Category'].value_counts()\n",
        "                print(f\"   Transaction categories: {len(category_counts)}\")\n",
        "                print(f\"   Most common: {category_counts.head(3).to_dict()}\")\n",
        "\n",
        "                # Find rare categories\n",
        "                rare_categories = category_counts[category_counts <= 2]\n",
        "                if len(rare_categories) > 0:\n",
        "                    rare_transactions = self.df[self.df['Category'].isin(rare_categories.index)]\n",
        "                    results['rare_category_transactions'] = rare_transactions\n",
        "                    print(f\"   Rare category transactions: {len(rare_transactions)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   Error analyzing patterns: {e}\")\n",
        "\n",
        "        # 6. Financial-specific anomalies\n",
        "        print(f\"\\n6. Financial-Specific Anomaly Detection\")\n",
        "        try:\n",
        "            fin_anomalies = self.financial_transaction_anomalies()\n",
        "            results['financial_anomalies'] = fin_anomalies\n",
        "\n",
        "            for anomaly_type, anomaly_data in fin_anomalies.items():\n",
        "                if isinstance(anomaly_data, pd.DataFrame):\n",
        "                    print(f\"   {anomaly_type.replace('_', ' ').title()}: {len(anomaly_data)} anomalies\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Error in financial anomaly detection: {e}\")\n",
        "\n",
        "        # 7. Time-based analysis (if DateTime available)\n",
        "        if 'DateTime' in self.df.columns and self.df['DateTime'].notna().sum() > 0:\n",
        "            print(f\"\\n7. Time-Based Analysis\")\n",
        "            try:\n",
        "                # Hour-based patterns\n",
        "                hour_counts = self.df['DateTime'].dt.hour.value_counts().sort_index()\n",
        "                print(f\"   Time range: {hour_counts.index.min()}:00 to {hour_counts.index.max()}:00\")\n",
        "\n",
        "                # Find unusual timing\n",
        "                unusual_hours = hour_counts[hour_counts <= 1]  # Very few transactions\n",
        "                if len(unusual_hours) > 0:\n",
        "                    unusual_time_transactions = self.df[self.df['DateTime'].dt.hour.isin(unusual_hours.index)]\n",
        "                    results['unusual_time_transactions'] = unusual_time_transactions\n",
        "                    print(f\"   Unusual timing transactions: {len(unusual_time_transactions)}\")\n",
        "\n",
        "                # Weekday vs Weekend analysis\n",
        "                if 'is_weekend' in self.df.columns:\n",
        "                    weekday_count = (~self.df['is_weekend']).sum()\n",
        "                    weekend_count = self.df['is_weekend'].sum()\n",
        "                    weekend_ratio = weekend_count / (weekday_count + weekend_count)\n",
        "                    print(f\"   Weekday transactions: {weekday_count} ({(1-weekend_ratio)*100:.1f}%)\")\n",
        "                    print(f\"   Weekend transactions: {weekend_count} ({weekend_ratio*100:.1f}%)\")\n",
        "                    print(f\"Weekend ratio(w.r.t. week):{weekend_ratio}\")\n",
        "                    print(f\"   Expected weekend ratio: 0.35-0.45\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Error in time analysis: {e}\")\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "0Qpoe0uWM23_"
      },
      "id": "0Qpoe0uWM23_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_financial_data(query: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Analyze financial data using the SQLAnomalyDetector with detailed category, month, and mode splits\n",
        "\n",
        "    Args:\n",
        "        query (str): Optional SQL query to filter data\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted analysis results with detailed breakdowns\n",
        "    \"\"\"\n",
        "    try:\n",
        "        detector = SQLAnomalyDetector(\"mydb.db\")\n",
        "\n",
        "        # Load data\n",
        "        df = detector.load_data(query)\n",
        "\n",
        "        if df is None or len(df) == 0:\n",
        "            return \"No data found for analysis\"\n",
        "\n",
        "        # Filter for positive amounts only (expenses/spending)\n",
        "        df_positive = df[df['Amount'] > 0].copy()\n",
        "\n",
        "        if len(df_positive) == 0:\n",
        "            return \"No positive amount transactions found for analysis\"\n",
        "\n",
        "        # Run comprehensive analysis on positive amounts only\n",
        "        results = detector.comprehensive_analysis()\n",
        "\n",
        "        # Initialize detailed analysis string to capture all insights\n",
        "        detailed_analysis = f\"\"\"\n",
        "📊 COMPREHENSIVE FINANCIAL DATA ANALYSIS (Positive Amounts Only)\n",
        "================================================================\n",
        "\n",
        "Dataset Overview:\n",
        "- Total records (all): {len(df)}\n",
        "- Positive amount records: {len(df_positive)}\n",
        "- Date range: {df_positive['Date'].min() if 'Date' in df_positive.columns else 'N/A'} to {df_positive['Date'].max() if 'Date' in df_positive.columns else 'N/A'}\n",
        "- Columns: {list(df_positive.columns)}\n",
        "\"\"\"\n",
        "\n",
        "        # Use df_positive for all subsequent calculations\n",
        "        df = df_positive.copy()\n",
        "\n",
        "        # === CATEGORY ANALYSIS ===\n",
        "        if 'Category' in df.columns:\n",
        "            detailed_analysis += \"\\n\\n🏷️ CATEGORY BREAKDOWN:\\n\" + \"=\"*50 + \"\\n\"\n",
        "\n",
        "            # category-counts split\n",
        "            category_stats = df.groupby('Category').agg({\n",
        "                'Amount': ['count', 'sum', 'mean', 'std', 'min', 'max']\n",
        "            }).round(2)\n",
        "            category_count_df = category_stats['Amount']['count'].reset_index()\n",
        "            category_count_df.columns = ['Category', 'Transaction Count']\n",
        "\n",
        "            # category-amount splits (now all amounts are positive)\n",
        "            if 'Amount' in df.columns:\n",
        "                # Remove the Money category filter since we're only dealing with positive amounts\n",
        "                category_amounts = df.groupby('Category')['Amount'].agg(['sum', 'mean', 'count']).round(2)\n",
        "                category_amounts = category_amounts.sort_values('sum', ascending=False)\n",
        "\n",
        "                # Capture the insights\n",
        "                detailed_analysis += f\"\\n   Amount spent by category:\\n\"\n",
        "                detailed_analysis += f\"   Top 5 categories by total amount:\\n\"\n",
        "                for cat, row in category_amounts.head(5).iterrows():\n",
        "                    detailed_analysis += f\"     {cat}: Rs.{row['sum']:,.2f} (avg: Rs.{row['mean']:,.2f}, {row['count']} transactions)\\n\"\n",
        "\n",
        "                # Create and display plot\n",
        "                plt.figure(figsize=(12, 8))\n",
        "\n",
        "                # Since all amounts are positive, we can simplify the plotting\n",
        "                bars = plt.bar(range(len(category_amounts)),\n",
        "                              category_amounts['sum'],\n",
        "                              color='green', alpha=0.7)\n",
        "\n",
        "                plt.xticks(range(len(category_amounts)),\n",
        "                          category_amounts.index,\n",
        "                          rotation=45, ha='right')\n",
        "                plt.title('Total Amount Spent per Category (Positive Amounts Only):', fontsize=16, pad=20)\n",
        "                plt.ylabel('Total Amount (Rs.)', fontsize=12)\n",
        "                plt.xlabel('Category', fontsize=12)\n",
        "\n",
        "                # Add value labels on bars\n",
        "                for i, (bar, value) in enumerate(zip(bars, category_amounts['sum'])):\n",
        "                    height = bar.get_height()\n",
        "                    plt.text(bar.get_x() + bar.get_width()/2., height + (height * 0.01),\n",
        "                            f'Rs.{value:,.0f}', ha='center', va='bottom',\n",
        "                            fontsize=10, rotation=0)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            # Calculate totals for percentages\n",
        "            total_transactions = len(df)\n",
        "            total_amount = df['Amount'].sum() if 'Amount' in df.columns else 0\n",
        "\n",
        "            detailed_analysis += f\"\\nTotal Categories: {len(category_stats)}\\n\"\n",
        "            detailed_analysis += f\"Total Transactions: {total_transactions}\\n\"\n",
        "            detailed_analysis += f\"Total Amount: Rs.{total_amount:,.2f}\\n\\n\"\n",
        "\n",
        "            # Update column names for the stats\n",
        "            category_stats.columns = ['Count', 'Total_Amount', 'Avg_Amount', 'Std_Amount', 'Min_Amount', 'Max_Amount']\n",
        "\n",
        "            detailed_analysis += \"📈 Category Statistics:\\n\"\n",
        "            for category, stats in category_stats.iterrows():\n",
        "                percentage = (stats['Count'] / total_transactions) * 100\n",
        "                amount_percentage = (stats['Total_Amount'] / total_amount) * 100 if total_amount > 0 else 0\n",
        "                detailed_analysis += f\"\\n  {category}:\\n\"\n",
        "                detailed_analysis += f\"    • Transactions: {stats['Count']} ({percentage:.1f}% of total)\\n\"\n",
        "                detailed_analysis += f\"    • Total Amount: Rs.{stats['Total_Amount']:,.2f} ({amount_percentage:.1f}% of total)\\n\"\n",
        "                detailed_analysis += f\"    • Average Amount: Rs.{stats['Avg_Amount']:,.2f}\\n\"\n",
        "                detailed_analysis += f\"    • Amount Range: Rs.{stats['Min_Amount']:,.2f} to Rs.{stats['Max_Amount']:,.2f}\\n\"\n",
        "                detailed_analysis += f\"    • Std Deviation: Rs.{stats['Std_Amount']:,.2f}\\n\"\n",
        "\n",
        "        # === MONTHLY ANALYSIS ===\n",
        "        if 'Date' in df.columns:\n",
        "            detailed_analysis += \"\\n\\n📅 MONTHLY BREAKDOWN:\\n\" + \"=\"*50 + \"\\n\"\n",
        "\n",
        "            try:\n",
        "                # Convert Date to datetime and extract month-year\n",
        "                df['Date_parsed'] = pd.to_datetime(df['Date'], format='%d-%b-%y', errors='coerce')\n",
        "                df['Month_Year'] = df['Date_parsed'].dt.to_period('M')\n",
        "\n",
        "                monthly_stats = df.groupby('Month_Year').agg({\n",
        "                    'Amount': ['count', 'sum', 'mean', 'std', 'min', 'max']\n",
        "                }).round(2)\n",
        "\n",
        "                monthly_stats.columns = ['Count', 'Total_Amount', 'Avg_Amount', 'Std_Amount', 'Min_Amount', 'Max_Amount']\n",
        "                monthly_stats = monthly_stats.sort_index()\n",
        "\n",
        "                detailed_analysis += f\"Date Range: {monthly_stats.index.min()} to {monthly_stats.index.max()}\\n\"\n",
        "                detailed_analysis += f\"Total Months: {len(monthly_stats)}\\n\\n\"\n",
        "\n",
        "                detailed_analysis += \"📊 Monthly Statistics:\\n\"\n",
        "                for month, stats in monthly_stats.iterrows():\n",
        "                    detailed_analysis += f\"\\n  {month}:\\n\"\n",
        "                    detailed_analysis += f\"    • Transactions: {stats['Count']}\\n\"\n",
        "                    detailed_analysis += f\"    • Total Amount: Rs.{stats['Total_Amount']:,.2f}\\n\"\n",
        "                    detailed_analysis += f\"    • Average Amount: Rs.{stats['Avg_Amount']:,.2f}\\n\"\n",
        "                    detailed_analysis += f\"    • Amount Range: Rs.{stats['Min_Amount']:,.2f} to Rs.{stats['Max_Amount']:,.2f}\\n\"\n",
        "                    detailed_analysis += f\"    • Std Deviation: Rs.{stats['Std_Amount']:,.2f}\\n\"\n",
        "\n",
        "                # Monthly trends\n",
        "                detailed_analysis += \"\\n📈 Monthly Trends:\\n\"\n",
        "                avg_monthly_amount = monthly_stats['Total_Amount'].mean()\n",
        "                highest_month = monthly_stats['Total_Amount'].idxmax()\n",
        "                lowest_month = monthly_stats['Total_Amount'].idxmin()\n",
        "\n",
        "                detailed_analysis += f\"  • Average Monthly Total: Rs.{avg_monthly_amount:,.2f}\\n\"\n",
        "                detailed_analysis += f\"  • Highest Activity: {highest_month} (Rs.{monthly_stats.loc[highest_month, 'Total_Amount']:,.2f})\\n\"\n",
        "                detailed_analysis += f\"  • Lowest Activity: {lowest_month} (Rs.{monthly_stats.loc[lowest_month, 'Total_Amount']:,.2f})\\n\"\n",
        "\n",
        "            except Exception as e:\n",
        "                detailed_analysis += f\"Error processing monthly data: {str(e)}\\n\"\n",
        "\n",
        "        # === PAYMENT MODE ANALYSIS ===\n",
        "        if 'Mode' in df.columns:\n",
        "            detailed_analysis += \"\\n\\n💳 PAYMENT MODE BREAKDOWN:\\n\" + \"=\"*50 + \"\\n\"\n",
        "\n",
        "            mode_stats = df.groupby('Mode').agg({\n",
        "                'Amount': ['count', 'sum', 'mean', 'std', 'min', 'max']\n",
        "            }).round(2)\n",
        "\n",
        "            mode_stats.columns = ['Count', 'Total_Amount', 'Avg_Amount', 'Std_Amount', 'Min_Amount', 'Max_Amount']\n",
        "            mode_stats = mode_stats.sort_values('Total_Amount', ascending=False)\n",
        "\n",
        "            detailed_analysis += f\"Total Payment Modes: {len(mode_stats)}\\n\"\n",
        "            detailed_analysis += f\"Most Used Mode: {mode_stats.index[0]} ({mode_stats.iloc[0]['Count']} transactions)\\n\"\n",
        "            detailed_analysis += f\"Highest Value Mode: {mode_stats['Total_Amount'].idxmax()}\\n\\n\"\n",
        "\n",
        "            detailed_analysis += \"💰 Payment Mode Statistics:\\n\"\n",
        "            for mode, stats in mode_stats.iterrows():\n",
        "                percentage = (stats['Count'] / total_transactions) * 100\n",
        "                amount_percentage = (stats['Total_Amount'] / total_amount) * 100 if total_amount > 0 else 0\n",
        "                detailed_analysis += f\"\\n  {mode}:\\n\"\n",
        "                detailed_analysis += f\"    • Transactions: {stats['Count']} ({percentage:.1f}% of total)\\n\"\n",
        "                detailed_analysis += f\"    • Total Amount: Rs.{stats['Total_Amount']:,.2f} ({amount_percentage:.1f}% of total)\\n\"\n",
        "                detailed_analysis += f\"    • Average Amount: Rs.{stats['Avg_Amount']:,.2f}\\n\"\n",
        "                detailed_analysis += f\"    • Amount Range: Rs.{stats['Min_Amount']:,.2f} to Rs.{stats['Max_Amount']:,.2f}\\n\"\n",
        "                detailed_analysis += f\"    • Std Deviation: Rs.{stats['Std_Amount']:,.2f}\\n\"\n",
        "\n",
        "        # === CROSS-ANALYSIS ===\n",
        "        detailed_analysis += \"\\n\\n🔄 CROSS-ANALYSIS:\\n\" + \"=\"*50 + \"\\n\"\n",
        "\n",
        "        # Category vs Mode analysis\n",
        "        if 'Category' in df.columns and 'Mode' in df.columns:\n",
        "            detailed_analysis += \"🏷️💳 Category vs Payment Mode:\\n\"\n",
        "            cross_analysis = df.groupby(['Category', 'Mode']).agg({\n",
        "                'Amount': ['count', 'sum', 'mean']\n",
        "            }).round(2)\n",
        "\n",
        "            cross_analysis.columns = ['Count', 'Total_Amount', 'Avg_Amount']\n",
        "\n",
        "            for category in df['Category'].unique():\n",
        "                if category in cross_analysis.index:\n",
        "                    cat_data = cross_analysis.loc[category]\n",
        "                    if isinstance(cat_data, pd.Series):\n",
        "                        cat_data = cat_data.to_frame().T\n",
        "                    if len(cat_data) > 0:\n",
        "                        detailed_analysis += f\"\\n  {category}:\\n\"\n",
        "                        for mode_idx, stats in cat_data.iterrows():\n",
        "                            mode_name = mode_idx if isinstance(mode_idx, str) else mode_idx\n",
        "                            detailed_analysis += f\"    • {mode_name}: {stats['Count']} transactions, Total: Rs.{stats['Total_Amount']:,.2f}\\n\"\n",
        "\n",
        "        # === ANOMALY DETECTION RESULTS ===\n",
        "        detailed_analysis += \"\\n\\n🚨 ANOMALY DETECTION RESULTS:\\n\" + \"=\"*50 + \"\\n\"\n",
        "\n",
        "        total_anomalies = 0\n",
        "        for key, value in results.items():\n",
        "            if isinstance(value, dict):\n",
        "                if key == 'financial_anomalies':\n",
        "                    detailed_analysis += f\"\\n{key.replace('_', ' ').title()}:\\n\"\n",
        "                    for anomaly_type, anomaly_data in value.items():\n",
        "                        if isinstance(anomaly_data, pd.DataFrame):\n",
        "                            anomaly_count = len(anomaly_data)\n",
        "                            total_anomalies += anomaly_count\n",
        "                            detailed_analysis += f\"  • {anomaly_type.replace('_', ' ').title()}: {anomaly_count} anomalies\\n\"\n",
        "\n",
        "                            # Show sample anomalies\n",
        "                            if anomaly_count > 0:\n",
        "                                detailed_analysis += \"    Sample anomalies:\\n\"\n",
        "                                for i, (idx, row) in enumerate(anomaly_data.head(3).iterrows()):\n",
        "                                    detailed_analysis += f\"      Row {idx}: \"\n",
        "                                    if 'Amount' in row:\n",
        "                                        detailed_analysis += f\"Amount: Rs.{row['Amount']:,.2f}, \"\n",
        "                                    if 'Category' in row:\n",
        "                                        detailed_analysis += f\"Category: {row['Category']}, \"\n",
        "                                    if 'Mode' in row:\n",
        "                                        detailed_analysis += f\"Mode: {row['Mode']}\"\n",
        "                                    detailed_analysis += \"\\n\"\n",
        "\n",
        "                elif 'mean' in str(value):\n",
        "                    detailed_analysis += f\"\\n{key.replace('_', ' ').title()}:\\n\"\n",
        "                    if hasattr(value, 'items'):\n",
        "                        for stat_key, stat_value in value.items():\n",
        "                            detailed_analysis += f\"  • {stat_key}: {stat_value}\\n\"\n",
        "\n",
        "            elif isinstance(value, pd.DataFrame):\n",
        "                anomaly_count = len(value)\n",
        "                total_anomalies += anomaly_count\n",
        "                detailed_analysis += f\"\\n{key.replace('_', ' ').title()}: {anomaly_count} anomalies detected\\n\"\n",
        "\n",
        "                if anomaly_count > 0:\n",
        "                    detailed_analysis += \"  Sample anomalies:\\n\"\n",
        "                    for i, (idx, row) in enumerate(value.head(3).iterrows()):\n",
        "                        detailed_analysis += f\"    • Row {idx}: \"\n",
        "                        if 'Amount' in row:\n",
        "                            detailed_analysis += f\"Amount: Rs.{row['Amount']:,.2f}, \"\n",
        "                        if 'Category' in row:\n",
        "                            detailed_analysis += f\"Category: {row['Category']}, \"\n",
        "                        if 'Mode' in row:\n",
        "                            detailed_analysis += f\"Mode: {row['Mode']}\"\n",
        "                        detailed_analysis += \"\\n\"\n",
        "\n",
        "        # === SUMMARY INSIGHTS ===\n",
        "        detailed_analysis += \"\\n\\n💡 KEY INSIGHTS:\\n\" + \"=\"*50 + \"\\n\"\n",
        "\n",
        "        if 'Category' in df.columns:\n",
        "            top_category = df['Category'].value_counts().index[0]\n",
        "            detailed_analysis += f\"• Most frequent category: {top_category}\\n\"\n",
        "\n",
        "        if 'Mode' in df.columns:\n",
        "            top_mode = df['Mode'].value_counts().index[0]\n",
        "            detailed_analysis += f\"• Most used payment mode: {top_mode}\\n\"\n",
        "\n",
        "        if 'Amount' in df.columns:\n",
        "            avg_amount = df['Amount'].mean()\n",
        "            detailed_analysis += f\"• Average transaction amount: Rs.{avg_amount:,.2f}\\n\"\n",
        "\n",
        "        detailed_analysis += f\"• Total anomalies detected: {total_anomalies}\\n\"\n",
        "\n",
        "        # Create spending over time plot (now only positive amounts)\n",
        "        df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y', errors='coerce')\n",
        "        daily = df.set_index('Date').resample('D')['Amount'].sum().fillna(0)\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        sns.lineplot(data=daily)\n",
        "        plt.title('Daily Spending Over Time (Positive Amounts Only)')\n",
        "        plt.ylabel('Daily Spending Amount (Rs.)')\n",
        "        plt.xlabel('Date')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return detailed_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error in financial analysis: {str(e)}\"\n",
        "\n",
        "\n",
        "def finance_analysis(user_query: str) -> str:\n",
        "    \"\"\"\n",
        "    Combined finance analysis function that queries database and performs analysis\n",
        "\n",
        "    Args:\n",
        "        user_query (str): User's financial query\n",
        "\n",
        "    Returns:\n",
        "        str: Comprehensive financial analysis and recommendations\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"📊 Starting Financial Analysis...\")\n",
        "\n",
        "        # Step 1: Query database for transaction data\n",
        "        print(\"🔍 Step 1: Querying transaction database...\")\n",
        "        db_query = \"SELECT * FROM transactions\"\n",
        "        db_result = query_db(db_query)\n",
        "\n",
        "        # Step 2: Perform comprehensive statistical analysis\n",
        "        print(\"📈 Step 2: Performing comprehensive statistical analysis...\")\n",
        "        analysis_result = analyze_financial_data()\n",
        "\n",
        "        # Step 3: Generate final report\n",
        "        print(\"💡 Step 3: Generating financial insights...\")\n",
        "\n",
        "        combined_analysis = f\"\"\"\n",
        "🏦 COMPREHENSIVE FINANCIAL ANALYSIS REPORT\n",
        "===========================================\n",
        "\n",
        "USER QUERY: {user_query}\n",
        "\n",
        "{analysis_result}\n",
        "\n",
        "📋 RECOMMENDATIONS:\n",
        "==================\n",
        "Based on the analysis above, here are key recommendations for your financial management:\n",
        "\n",
        "1. Review your top spending categories and identify areas for potential savings\n",
        "2. Monitor anomalies regularly to catch unusual spending patterns\n",
        "3. Consider balancing payment modes for better tracking and rewards optimization\n",
        "4. Track monthly trends to identify seasonal spending patterns\n",
        "5. Set budgets for categories with high variability\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        return combined_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error in finance analysis: {str(e)}\""
      ],
      "metadata": {
        "id": "WYjtFFShTLXG"
      },
      "id": "WYjtFFShTLXG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finance_agent = AssistantAgent(\n",
        "    name=\"finance_agent\",\n",
        "    model_client=model_client,\n",
        "    tools=[query_db],\n",
        "    system_message=\"\"\"You are a senior financial advisor and analyst. Your role is to:\n",
        "\n",
        "1. **Assess Query Complexity**: Determine if the user query requires basic data retrieval or complex statistical analysis\n",
        "2. **Handle Basic Queries**: For simple data requests, use query_db to get information and provide basic financial insights\n",
        "3. **Coordinate Complex Analysis**: For complex queries requiring statistical analysis, anomaly detection, or pattern recognition:\n",
        "   - First use query_db to understand the data structure\n",
        "   - Then request the data analyst to perform comprehensive analysis\n",
        "   - Wait for data analyst's statistical insights\n",
        "   - Combine their findings with additional data queries if needed\n",
        "4. **Provide Final Recommendations**: Give actionable financial advice based on SPECIFIC STATISTICAL FINDINGS\n",
        "\n",
        "**WHEN TO INVOLVE DATA ANALYST**:\n",
        "- User asks for \"analysis\", \"patterns\", \"trends\", \"anomalies\", or \"insights\"\n",
        "- Requests about spending behavior, risk assessment, or budget optimization\n",
        "- Questions requiring statistical analysis or data science techniques\n",
        "- Complex queries that need more than basic data retrieval\n",
        "\n",
        "**BASIC QUERIES** (handle yourself with query_db):\n",
        "- \"Show me recent transactions\"\n",
        "- \"What's my current balance?\"\n",
        "- \"List transactions by category\"\n",
        "- Simple data retrieval requests\n",
        "\n",
        "**COMPLEX QUERIES** (involve data analyst):\n",
        "- \"Analyze my spending patterns\"\n",
        "- \"Find unusual transactions\"\n",
        "- \"What are my financial trends?\"\n",
        "- \"Provide budget recommendations\"\n",
        "\n",
        "**CRITICAL - USE SPECIFIC DATA FROM ANALYSIS**:\n",
        "When data analyst provides statistical insights, use the ACTUAL NUMBERS and SPECIFIC FINDINGS:\n",
        "- Reference exact amounts, percentages, and counts from their analysis\n",
        "- Quote specific statistics like \"Your average spending is Rs.X with Rs.Y standard deviation\"\n",
        "- Use actual category breakdowns like \"Food represents 270 out of 564 transactions (48%)\"\n",
        "- Reference specific anomaly counts and what they mean financially\n",
        "- Base recommendations on the actual data patterns found\n",
        "\n",
        "**IMPORTANT**:\n",
        "- After data analyst provides statistical insights, interpret the SPECIFIC NUMBERS and provide targeted recommendations\n",
        "- Don't provide generic advice - use the actual statistical findings\n",
        "- Always provide financial interpretation and recommendations based on real data\n",
        "- Reference specific anomalies, spending patterns, and statistical measures\n",
        "\n",
        "**YOUR FINAL RESPONSE SHOULD INCLUDE**:\n",
        "- Interpretation of specific statistical findings (using actual numbers)\n",
        "- Targeted recommendations based on the real spending patterns found\n",
        "- Risk assessment based on actual anomaly counts and patterns\n",
        "- Specific next steps based on the data analysis results\"\"\",\n",
        "    reflect_on_tool_use=True,\n",
        ")\n",
        "\n",
        "data_analyst = AssistantAgent(\n",
        "    name=\"data_analyst\",\n",
        "    model_client=model_client,\n",
        "    tools=[analyze_financial_data],\n",
        "    system_message=\"\"\"You are a senior data analyst specializing in financial data analysis. Your role is to:\n",
        "\n",
        "1. **Receive Analysis Requests**: Finance agent will request statistical analysis for complex queries\n",
        "2. **Perform Comprehensive Analysis**: Use analyze_financial_data function to run detailed statistical analysis\n",
        "3. **Provide Statistical Insights**: Return detailed findings with ACTUAL NUMBERS and SPECIFIC DATA from the analysis\n",
        "\n",
        "**YOUR ANALYSIS MUST INCLUDE ACTUAL VALUES FROM THE ANALYSIS RESULT**:\n",
        "- Use the specific numbers returned by the analyze_financial_data function\n",
        "- Quote exact statistics (mean, median, std dev, ranges) from the analysis\n",
        "- Reference specific category counts and payment mode statistics\n",
        "- Include actual anomaly counts and outlier details\n",
        "- Provide real insights based on the returned data, not generic placeholders\n",
        "\n",
        "**EXAMPLE OF PROPER ANALYSIS**:\n",
        "\"Based on the analysis results:\n",
        "- Amount statistics: Mean: -0.49, Std Dev: 3826.63, Range: -61100 to 65044\n",
        "- Transaction categories: 14 total, with Food (270 transactions), Drink (119), Travel (39) as top categories\n",
        "- Payment modes: Google Pay (436 transactions), Cash (56), Paytm (30)\n",
        "- Anomalies detected: 63 IQR outliers, 55 multi-dimensional anomalies, 38 cash withdrawal anomalies\"\n",
        "\n",
        "**IMPORTANT**:\n",
        "- NEVER use placeholders like \"(Specify Mean)\" or \"(Provide descriptive stats here)\"\n",
        "- ALWAYS extract and report the actual numerical values from the analysis results\n",
        "- Base your insights on the real data returned by the analyze_financial_data function\n",
        "- Be specific about what the numbers mean statistically\n",
        "\n",
        "**RESPONSE FORMAT**:\n",
        "After analysis, provide comprehensive statistical findings with ACTUAL DATA and conclude with:\n",
        "\"Finance agent, please interpret these SPECIFIC statistical insights and provide financial recommendations to the user.\"\n",
        "\"\"\",\n",
        "    reflect_on_tool_use=True,\n",
        ")\n",
        "\n",
        "from autogen_agentchat.teams import SelectorGroupChat\n",
        "\n",
        "group_chat = SelectorGroupChat(\n",
        "    participants=[finance_agent, data_analyst],\n",
        "    model_client=model_client,\n",
        ")\n",
        "\n",
        "async def run_finance_analysis(user_query: str):\n",
        "    \"\"\"\n",
        "    Run the two-agent financial analysis workflow with live conversation display\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"🚀 Starting Two-Agent Financial Analysis...\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        task = f\"\"\"\n",
        "USER QUERY: {user_query}\n",
        "\n",
        "WORKFLOW INSTRUCTIONS:\n",
        "1. Finance Agent: Assess if this is a basic query or requires complex analysis\n",
        "2. For BASIC queries: Finance agent uses query_db and provides simple insights\n",
        "3. For COMPLEX queries:\n",
        "   - Finance agent requests data analyst to perform statistical analysis\n",
        "   - Data analyst uses analyze_financial_data function and returns statistical insights\n",
        "   - Finance agent MUST interpret the statistical insights and provide final financial recommendations\n",
        "\n",
        "GOAL: Provide comprehensive financial analysis with actionable recommendations.\n",
        "\"\"\"\n",
        "\n",
        "        # Create an async iterator to show conversation in real-time\n",
        "        async for message in group_chat.run_stream(task=task):\n",
        "            if hasattr(message, 'source') and hasattr(message, 'content'):\n",
        "                if message.source == 'finance_agent':\n",
        "                    print(f\"\\n🏦 FINANCE AGENT:\")\n",
        "                    print(\"-\" * 40)\n",
        "                    print(f\"{message.content}\")\n",
        "                    print(\"-\" * 40)\n",
        "                elif message.source == 'data_analyst':\n",
        "                    print(f\"\\n📊 DATA ANALYST:\")\n",
        "                    print(\"-\" * 40)\n",
        "                    print(f\"{message.content}\")\n",
        "                    print(\"-\" * 40)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"✅ Analysis Complete!\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in financial analysis: {e}\")\n",
        "        return False\n",
        "\n",
        "async def main():\n",
        "    test_queries = [\n",
        "        \"Analyze my spending patterns and provide budget recommendations\",  # Complex\n",
        "        \"Show me my recent transactions\",  # Basic\n",
        "        \"What are my financial trends and any anomalies?\",  # Complex\n",
        "        \"List my transactions by category\",  # Basic\n",
        "        \"Perform comprehensive analysis of my financial data\",  # Complex\n",
        "        \"What's my current balance trend?\"  # Basic\n",
        "    ]\n",
        "\n",
        "    query = input(\"What would you like to know about your financial data? \") or test_queries[0]\n",
        "\n",
        "    success = await run_finance_analysis(query)\n",
        "\n",
        "    if not success:\n",
        "        print(\"❌ Analysis failed. Please check your setup and try again.\")\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "id": "Ch3y3OxH5vOH"
      },
      "id": "Ch3y3OxH5vOH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_financial_data()"
      ],
      "metadata": {
        "id": "6ICjiATAnra-"
      },
      "id": "6ICjiATAnra-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}